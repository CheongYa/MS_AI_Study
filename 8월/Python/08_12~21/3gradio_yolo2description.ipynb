{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels Length:  80\n",
      "Running on local URL:  http://127.0.0.1:7879\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7879/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "################################### YOLO V3 ###################################\n",
    "\n",
    "# Yolov3 파일 경로\n",
    "weights_path = 'yolo3/yolov3.weights'\n",
    "config_paht = 'yolo3/yolov3.cfg'\n",
    "names_paht = 'yolo3/coco.names'\n",
    "\n",
    "# Yolov3 모델 로드\n",
    "net = cv2.dnn.readNet(weights_path, config_paht)\n",
    "\n",
    "# 라벨 이름 로드\n",
    "with open(names_paht, 'r') as f:\n",
    "    labels = f.read().strip().split('\\n')\n",
    "    print('Labels Length: ', len(labels))\n",
    "\n",
    "# 객체 감지 함수\n",
    "def detect_objects(image):\n",
    "    height, width = image.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(image, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layer_names = net.getLayerNames()\n",
    "\n",
    "    output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    # print(output_layers)\n",
    "    detections = net.forward(output_layers)\n",
    "    # print(detections)\n",
    "\n",
    "    box_list = []\n",
    "    confidence_list = []\n",
    "    class_id_list = []\n",
    "\n",
    "    for output in detections:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.5:\n",
    "                box = detection[0:4] * np.array([width, height, width, height])\n",
    "                (center_x, center_y, w, h) = box.astype(\"int\")\n",
    "                x = int(center_x - (w / 2))\n",
    "                y = int(center_y - (h / 2))\n",
    "\n",
    "                box_list.append([x, y, int(w), int(h)])\n",
    "                confidence_list.append(float(confidence))\n",
    "                class_id_list.append(class_id)\n",
    "    \n",
    "    index_list = cv2.dnn.NMSBoxes(box_list, confidence_list, 0.5, 0.4)\n",
    "\n",
    "    if len(index_list) > 0:\n",
    "        for i in index_list.flatten():\n",
    "            x, y, w, h = box_list[i]\n",
    "            label = str(labels[class_id_list[i]])\n",
    "            confidence = confidence_list[i]\n",
    "\n",
    "            # 사각형 그리기\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "            # 레이블 표시\n",
    "            cv2.putText(image, f\"{label} {confidence:.2f}\", (x, y - 10), cv2.FONT_HERSHEY_SCRIPT_COMPLEX, 2, (0, 0, 255), 2)\n",
    "\n",
    "    return image\n",
    "\n",
    "#########################################################################################################\n",
    "\n",
    "################################ chatgpt_response ################################\n",
    "\n",
    "def chatgpt_response(image_array, history):\n",
    "    endpoint = \"https://fimtrus-openai.openai.azure.com\"\n",
    "    api_key = \"310a6832c2394daf97a3f446cc86ce20\"\n",
    "    deployment_name = \"fitmrus-gpt4o\"\n",
    "\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'api-key': api_key\n",
    "    }\n",
    "\n",
    "    messages = []\n",
    "\n",
    "    # System\n",
    "    messages.append({\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\n",
    "            \"type\":\"text\",\n",
    "            \"text\": \"너는 사진 속에서 감지된 물체에 대해서 분석하는 봇이야.\"\n",
    "        }]\n",
    "    })\n",
    "\n",
    "    image = Image.fromarray(image_array)\n",
    "    buffered_io = BytesIO()\n",
    "    image.save(buffered_io, format='png')\n",
    "    base64_image = base64.b64encode(buffered_io.getvalue()).decode(\"utf-8\")\n",
    "    \n",
    "    # User\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\n",
    "            \"type\":\"text\",\n",
    "            \"text\": \"이 사진에서 감지된 물체에 대해 감지 확률과 함께 자세하게 설명해줘.\"\n",
    "        },{\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "            }\n",
    "        }]\n",
    "    })\n",
    "\n",
    "    payload = {\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_tokens\": 800\n",
    "    }\n",
    "\n",
    "    response = requests.post(\n",
    "        f\"{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version=2024-02-15-preview\",\n",
    "        headers=headers,\n",
    "        json=payload\n",
    "    )\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        bot_response = result['choices'][0]['message']['content'].strip()\n",
    "        history.append(('User', bot_response))\n",
    "        return history\n",
    "    else:\n",
    "        history.append((str(response.status_code), response.text))\n",
    "        return history\n",
    "\n",
    "##################################################################################\n",
    "\n",
    "################################### TTS ###################################\n",
    "def get_token():\n",
    "    endpoint = \"https://eastus.api.cognitive.microsoft.com/sts/v1.0/issueToken\"\n",
    "    api_key = \"8e5931bade634c4e859a8e7544f87ff7\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Ocp-Apim-Subscription-Key\": api_key,\n",
    "    }\n",
    "\n",
    "    response = requests.post(endpoint, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        token = response.text\n",
    "        return token\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def request_tts(text):\n",
    "    endpoint = \"https://eastus.tts.speech.microsoft.com/cognitiveservices/v1\"\n",
    "    token = get_token()\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/ssml+xml\",\n",
    "        \"User-Agent\": \"testForEducation\",\n",
    "        \"X-Microsoft-OutputFormat\": \"riff-24khz-16bit-mono-pcm\",\n",
    "        \"Authorization\": f\"Bearer {token}\"\n",
    "    }\n",
    "\n",
    "    data = f\"\"\"\n",
    "        <speak version='1.0' xml:lang='ko-KR'><voice xml:lang='ko-KR' xml:gender='Female' name='ko-KR-JiMinNeural'>\n",
    "            {text}\n",
    "        </voice></speak>\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.post(endpoint,\n",
    "                             headers=headers,\n",
    "                             data=data)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        file_name = 'response_audio.wav'\n",
    "        with open(file_name, \"wb\") as audio_file:\n",
    "            audio_file.write(response.content)\n",
    "        return file_name\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "###########################################################################\n",
    "\n",
    "def stream_webcam(image):\n",
    "    return detect_objects(image)\n",
    "\n",
    "def click_capture(image):\n",
    "    return image\n",
    "\n",
    "def click_send_gpt(image_array, history):\n",
    "    return chatgpt_response(image_array, history)\n",
    "\n",
    "def change_chatbot(chatbot):\n",
    "    # TTS\n",
    "    text = chatbot[-1][1]\n",
    "    import re\n",
    "    pattern = r'[^가-힣a-zA-Z0-9\\s]'\n",
    "    cleaned_text = re.sub(pattern, ' ', text)\n",
    "    file_name = request_tts(cleaned_text)\n",
    "    # speech file, wave 리턴\n",
    "    return file_name\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Fimtru's AI World!!!\")\n",
    "\n",
    "    with gr.Column():\n",
    "        # 실시간 화면, 실시간 감지, 캡쳐 화면\n",
    "        with gr.Row():\n",
    "            webcam_input = gr.Image(label=\"실시간 화면\", sources=\"webcam\")\n",
    "            output_image = gr.Image(label=\"실시간 감지\", interactive=False)\n",
    "            output_capture_image = gr.Image(label=\"캡쳐 화면\", interactive=False)\n",
    "\n",
    "        # 캡쳐 버튼, GPT로 전송한는 버튼\n",
    "        with gr.Row():\n",
    "            capture_button = gr.Button('캡처')\n",
    "            send_gpt_button = gr.Button('GPT')\n",
    "\n",
    "    with gr.Column():\n",
    "        # chatbot, audio\n",
    "        chatbot = gr.Chatbot(label=\"분석 결과\")\n",
    "        chatbot_audio = gr.Audio(label='GPT', interactive=False)\n",
    "    \n",
    "    webcam_input.stream(fn=stream_webcam, inputs=[webcam_input], outputs=[output_image])\n",
    "    capture_button.click(fn=click_capture, inputs=[output_image], outputs=[output_capture_image])\n",
    "    send_gpt_button.click(fn=click_send_gpt, inputs=[output_capture_image, chatbot], outputs=[chatbot])\n",
    "    chatbot.change(fn=change_chatbot, inputs=[chatbot], outputs=[chatbot_audio])\n",
    "    # 실시간 화면에 대한 stream event.\n",
    "    # 각종 이벤트 리스너 필요.\n",
    "\n",
    "demo.launch(share=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
